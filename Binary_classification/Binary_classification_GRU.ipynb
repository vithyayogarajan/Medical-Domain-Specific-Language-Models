{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary classification using GRU with general text pre-trained embeddings. \n",
    "\n",
    "### MIMIC-III discharge summary, for ICD-9 level 1 `circ' label where `circ' includes ICD-9 codes between 390-459: diseases of the circulatory system.\n",
    "### Percentage of occurrence is of `circ'in unique hospital admissions in MIMIC-III is 78.4%. \n",
    "### The total number of hospital admissions with a recorded discharge summary is 52,710."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "#import gensim\n",
    "import string\n",
    "import codecs\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "\n",
    "import tensorflow as tf\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding,LSTM,Dense,Dropout,GRU\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['.', ',', '\"', \"'\", ':', ';', '(', ')', '[', ']', '{', '}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>admission date discharge date date of birth se...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>admission date discharge date date of birth se...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>admission date discharge date date of birth se...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>admission date discharge date date of birth se...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>admission date discharge date date of birth se...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                TEXT  label\n",
       "0  admission date discharge date date of birth se...      1\n",
       "1  admission date discharge date date of birth se...      0\n",
       "2  admission date discharge date date of birth se...      1\n",
       "3  admission date discharge date date of birth se...      1\n",
       "4  admission date discharge date date of birth se...      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"circ_binary_mimic_dis.csv\")\n",
    "df = df.drop(['HADM_ID'],axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['split'] = np.random.randn(df.shape[0], 1)\n",
    "\n",
    "msk = np.random.rand(len(df)) <= 0.7\n",
    "train = df[msk]\n",
    "test = df[~msk]\n",
    "train = train.drop(['split'],axis=1)\n",
    "test = test.drop(['split'],axis=1)\n",
    "texts = train.TEXT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 270/36692 [00:00<00:13, 2685.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-processing train data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36692/36692 [00:14<00:00, 2585.24it/s]\n",
      "100%|██████████| 16030/16030 [00:06<00:00, 2554.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing input data...\n",
      "dictionary size:  150352\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "MAX_NB_WORDS = 100000\n",
    "max_seq_len = 3000\n",
    "\n",
    "raw_docs_train = train['TEXT'].tolist()\n",
    "raw_docs_test = test['TEXT'].tolist() \n",
    "num_classes = 1\n",
    "\n",
    "print(\"pre-processing train data...\")\n",
    "processed_docs_train = []\n",
    "for doc in tqdm(raw_docs_train):\n",
    "    tokens = tokenizer.tokenize(doc)\n",
    "    filtered = [word for word in tokens if word not in stop_words]\n",
    "    processed_docs_train.append(\" \".join(filtered))\n",
    "\n",
    "\n",
    "processed_docs_test = []\n",
    "for doc in tqdm(raw_docs_test):\n",
    "    tokens = tokenizer.tokenize(doc)\n",
    "    filtered = [word for word in tokens if word not in stop_words]\n",
    "    processed_docs_test.append(\" \".join(filtered))\n",
    "\n",
    "\n",
    "print(\"tokenizing input data...\")\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(processed_docs_train + processed_docs_test)  #leaky\n",
    "word_seq_train = tokenizer.texts_to_sequences(processed_docs_train)\n",
    "word_seq_test = tokenizer.texts_to_sequences(processed_docs_test)\n",
    "word_index = tokenizer.word_index\n",
    "print(\"dictionary size: \", len(word_index))\n",
    "\n",
    "#pad sequences\n",
    "word_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)\n",
    "word_seq_test = sequence.pad_sequences(word_seq_test, maxlen=max_seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1335it [00:00, 13347.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000001it [02:19, 14387.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 2000000 word vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#load pre-trained embeddings \n",
    "print('loading word embeddings...')\n",
    "embeddings_index = {}\n",
    "f = codecs.open('cc.en.300.vec', encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values = line.rstrip().rsplit(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('found %s word vectors' % len(embeddings_index))\n",
    "\n",
    "\n",
    "embed_dim = 300 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing embedding matrix...\n",
      "number of null word embeddings: 52460\n"
     ]
    }
   ],
   "source": [
    "#embedding matrix\n",
    "print('preparing embedding matrix...')\n",
    "words_not_found = []\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        words_not_found.append(word)\n",
    "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 3000, 300)         30000000  \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 128)               165120    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 30,165,249\n",
      "Trainable params: 165,249\n",
      "Non-trainable params: 30,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "model=Sequential()\n",
    "optimzer=Adam(clipvalue=0.5)\n",
    "\n",
    "embedding=Embedding(nb_words, embed_dim,\n",
    "          weights=[embedding_matrix], input_length=max_seq_len, trainable=False)\n",
    "\n",
    "model.add(embedding)\n",
    "model.add(GRU(128, return_sequences=False))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(optimizer=optimzer, loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "516/516 - 73s - loss: 0.2209 - acc: 0.9075 - val_loss: 0.2034 - val_acc: 0.9101\n",
      "Epoch 2/100\n",
      "516/516 - 73s - loss: 0.2111 - acc: 0.9122 - val_loss: 0.2058 - val_acc: 0.9079\n",
      "Epoch 3/100\n",
      "516/516 - 73s - loss: 0.2027 - acc: 0.9164 - val_loss: 0.2200 - val_acc: 0.8984\n",
      "Epoch 4/100\n",
      "516/516 - 73s - loss: 0.1948 - acc: 0.9190 - val_loss: 0.1931 - val_acc: 0.9144\n",
      "Epoch 5/100\n",
      "516/516 - 73s - loss: 0.1878 - acc: 0.9206 - val_loss: 0.2103 - val_acc: 0.9068\n",
      "Epoch 6/100\n",
      "516/516 - 73s - loss: 0.1832 - acc: 0.9233 - val_loss: 0.1912 - val_acc: 0.9169\n",
      "Epoch 7/100\n",
      "516/516 - 73s - loss: 0.1789 - acc: 0.9269 - val_loss: 0.1925 - val_acc: 0.9210\n",
      "Epoch 8/100\n",
      "516/516 - 73s - loss: 0.1734 - acc: 0.9289 - val_loss: 0.1917 - val_acc: 0.9172\n",
      "Epoch 9/100\n",
      "516/516 - 73s - loss: 0.1653 - acc: 0.9317 - val_loss: 0.2001 - val_acc: 0.9131\n",
      "Epoch 10/100\n",
      "516/516 - 73s - loss: 0.1613 - acc: 0.9331 - val_loss: 0.2002 - val_acc: 0.9158\n",
      "Epoch 11/100\n",
      "516/516 - 73s - loss: 0.1534 - acc: 0.9374 - val_loss: 0.2039 - val_acc: 0.9098\n",
      "Epoch 12/100\n",
      "516/516 - 73s - loss: 0.1490 - acc: 0.9392 - val_loss: 0.2051 - val_acc: 0.9114\n",
      "Epoch 13/100\n",
      "516/516 - 73s - loss: 0.1404 - acc: 0.9421 - val_loss: 0.2129 - val_acc: 0.9161\n",
      "Epoch 14/100\n",
      "516/516 - 73s - loss: 0.1329 - acc: 0.9467 - val_loss: 0.2237 - val_acc: 0.9117\n",
      "Epoch 15/100\n",
      "516/516 - 73s - loss: 0.1289 - acc: 0.9476 - val_loss: 0.2359 - val_acc: 0.9106\n",
      "Epoch 16/100\n",
      "516/516 - 73s - loss: 0.1175 - acc: 0.9541 - val_loss: 0.2440 - val_acc: 0.9082\n",
      "Epoch 17/100\n",
      "516/516 - 73s - loss: 0.1096 - acc: 0.9579 - val_loss: 0.2581 - val_acc: 0.9082\n",
      "Epoch 18/100\n",
      "516/516 - 73s - loss: 0.1028 - acc: 0.9603 - val_loss: 0.2694 - val_acc: 0.9046\n",
      "Epoch 19/100\n",
      "516/516 - 73s - loss: 0.0940 - acc: 0.9640 - val_loss: 0.2744 - val_acc: 0.9057\n",
      "Epoch 20/100\n",
      "516/516 - 73s - loss: 0.0881 - acc: 0.9666 - val_loss: 0.2858 - val_acc: 0.9011\n",
      "Epoch 21/100\n",
      "516/516 - 73s - loss: 0.0828 - acc: 0.9696 - val_loss: 0.2974 - val_acc: 0.9005\n",
      "Epoch 22/100\n",
      "516/516 - 73s - loss: 0.0734 - acc: 0.9728 - val_loss: 0.3200 - val_acc: 0.9011\n",
      "Epoch 23/100\n",
      "516/516 - 73s - loss: 0.0715 - acc: 0.9737 - val_loss: 0.3076 - val_acc: 0.9038\n",
      "Epoch 24/100\n",
      "516/516 - 73s - loss: 0.0685 - acc: 0.9759 - val_loss: 0.3565 - val_acc: 0.9014\n",
      "Epoch 25/100\n",
      "516/516 - 73s - loss: 0.0590 - acc: 0.9794 - val_loss: 0.3927 - val_acc: 0.8997\n",
      "Epoch 26/100\n",
      "516/516 - 73s - loss: 0.0560 - acc: 0.9798 - val_loss: 0.3470 - val_acc: 0.9005\n",
      "Epoch 27/100\n",
      "516/516 - 73s - loss: 0.0479 - acc: 0.9829 - val_loss: 0.3815 - val_acc: 0.8978\n",
      "Epoch 28/100\n",
      "516/516 - 73s - loss: 0.0457 - acc: 0.9844 - val_loss: 0.3962 - val_acc: 0.9057\n",
      "Epoch 29/100\n",
      "516/516 - 73s - loss: 0.0451 - acc: 0.9848 - val_loss: 0.3991 - val_acc: 0.8954\n",
      "Epoch 30/100\n",
      "516/516 - 73s - loss: 0.0346 - acc: 0.9883 - val_loss: 0.5283 - val_acc: 0.8967\n",
      "Epoch 31/100\n",
      "516/516 - 73s - loss: 0.0406 - acc: 0.9856 - val_loss: 0.4160 - val_acc: 0.8995\n",
      "Epoch 32/100\n",
      "516/516 - 73s - loss: 0.0331 - acc: 0.9884 - val_loss: 0.4202 - val_acc: 0.8962\n",
      "Epoch 33/100\n",
      "516/516 - 73s - loss: 0.0353 - acc: 0.9879 - val_loss: 0.4025 - val_acc: 0.9011\n",
      "Epoch 34/100\n",
      "516/516 - 73s - loss: 0.0297 - acc: 0.9902 - val_loss: 0.4767 - val_acc: 0.9025\n",
      "Epoch 35/100\n",
      "516/516 - 73s - loss: 0.0285 - acc: 0.9906 - val_loss: 0.4693 - val_acc: 0.9011\n",
      "Epoch 36/100\n",
      "516/516 - 73s - loss: 0.0242 - acc: 0.9919 - val_loss: 0.4872 - val_acc: 0.9041\n",
      "Epoch 37/100\n",
      "516/516 - 73s - loss: 0.0245 - acc: 0.9919 - val_loss: 0.4674 - val_acc: 0.8965\n",
      "Epoch 38/100\n",
      "516/516 - 73s - loss: 0.0310 - acc: 0.9903 - val_loss: 0.4583 - val_acc: 0.8932\n",
      "Epoch 39/100\n",
      "516/516 - 73s - loss: 0.0255 - acc: 0.9924 - val_loss: 0.4611 - val_acc: 0.8973\n"
     ]
    }
   ],
   "source": [
    "## Modify the number of epochs. This is only for an example.\n",
    "history=model.fit(word_seq_train,train.label, callbacks=callback, \n",
    "                  batch_size=64, epochs=100, \n",
    "                  validation_split=0.1, \n",
    "                  verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7788    0.8011    0.7898      3494\n",
      "           1     0.9441    0.9366    0.9403     12536\n",
      "\n",
      "    accuracy                         0.9070     16030\n",
      "   macro avg     0.8615    0.8688    0.8651     16030\n",
      "weighted avg     0.9081    0.9070    0.9075     16030\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_out = model.predict(word_seq_test,batch_size=64)\n",
    "y_pred = np.where(y_out > 0.5, 1, 0)\n",
    "\n",
    "\n",
    "print(classification_report(test.label, y_pred,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
