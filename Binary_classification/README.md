# Binary classification

## Flow chart of using bag-of-words for predictions
<img src="https://user-images.githubusercontent.com/60803118/131305709-fd98f95b-d610-4779-9f48-e4a6c89a8268.png" alt="BOW" width="500"/>

## Flow chart of using word embeddings for predictions  
<img src="https://user-images.githubusercontent.com/60803118/131305724-522b4c52-363e-4ae6-bfa8-11875963b5a1.png" alt="wordembeddings" width="500"/>

## Transformers
<img src="https://user-images.githubusercontent.com/60803118/131306468-1be56b09-286a-49f7-8bbe-d88993149ce9.png" alt="BERT" width="200"/>


Models used include:[BERT-base](https://github.com/google-research/bert),[Clinical BERT](https://github.com/EmilyAlsentzer/clinicalBERT),[BioMed-RoBERTa](https://huggingface.co/allenai/biomed_roberta_base),[PubMedBERT](https://microsoft.github.io/BLURB/models.html),[MeDAL-Electra](https://github.com/BruceWen120/medal),[Longformer](https://github.com/allenai/longformer) and [TransformerXL](https://github.com/kimiyoung/transformer-xl)

Transformer implementations are based on the open-source PyTorch-transformer repositories [Huggingface](https://github.com/huggingface/transformers) and [Simple Transformers](https://simpletransformers.ai/). 

Neural network models presented are implemented using [PyTorch](https://github.com/pytorch/pytorch) and [Keras/Tensorflow](https://www.tensorflow.org). 

Evaluations were done using [sklearn metrics](https://scikit-learn.org/stable/modules/classes.html\#module-sklearn.metrics). 


 

