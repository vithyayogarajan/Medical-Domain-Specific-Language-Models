{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "from operator import itemgetter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Input, MaxPooling1D, Convolution1D, Embedding\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df shape:  (52722, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT</th>\n",
       "      <th>blood</th>\n",
       "      <th>circulatory</th>\n",
       "      <th>congenital</th>\n",
       "      <th>digestive</th>\n",
       "      <th>endocrine</th>\n",
       "      <th>genitourinary</th>\n",
       "      <th>infectious</th>\n",
       "      <th>injury</th>\n",
       "      <th>mental</th>\n",
       "      <th>muscular</th>\n",
       "      <th>neoplasms</th>\n",
       "      <th>nervous</th>\n",
       "      <th>pregnancy</th>\n",
       "      <th>prenatal</th>\n",
       "      <th>respiratory</th>\n",
       "      <th>skin</th>\n",
       "      <th>symptoms</th>\n",
       "      <th>E and V</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>admission date discharge date date of birth se...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>admission date discharge date date of birth se...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>admission date discharge date date of birth se...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                TEXT  blood  circulatory  \\\n",
       "0  admission date discharge date date of birth se...      0            1   \n",
       "1  admission date discharge date date of birth se...      0            0   \n",
       "2  admission date discharge date date of birth se...      1            1   \n",
       "\n",
       "   congenital  digestive  endocrine  genitourinary  infectious  injury  \\\n",
       "0           0          0          1              1           1       0   \n",
       "1           0          1          1              0           1       0   \n",
       "2           0          0          1              0           0       1   \n",
       "\n",
       "   mental  muscular  neoplasms  nervous  pregnancy  prenatal  respiratory  \\\n",
       "0       0         0          0        0          0         0            0   \n",
       "1       0         0          0        0          0         0            0   \n",
       "2       0         0          0        0          0         0            0   \n",
       "\n",
       "   skin  symptoms  E and V  \n",
       "0     1         1        0  \n",
       "1     0         1        1  \n",
       "2     0         0        1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading file\n",
    "df = pd.read_csv('18label_mimic_dis.csv') ### pre-processed text with 18 labels\n",
    "print('df shape: ', df.shape)\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['TEXT', 'blood', 'circulatory', 'congenital', 'digestive', 'endocrine',\n",
       "       'genitourinary', 'infectious', 'injury', 'mental', 'muscular',\n",
       "       'neoplasms', 'nervous', 'pregnancy', 'prenatal', 'respiratory', 'skin',\n",
       "       'symptoms', 'E and V'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This file contains the functions necessary to vectorize the ICD labels and text inputs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Vectorize ICD codes\n",
    "\n",
    "def vectorize_icd_string(x, code_list):\n",
    "    \"\"\"Takes a string with ICD codes and returns an array of the right of 0/1\"\"\"\n",
    "    r = []\n",
    "    for code in code_list:\n",
    "        if code in x: r.append(1)\n",
    "        else: r.append(0)\n",
    "    return np.asarray(r)\n",
    "\n",
    "def vectorize_icd_column(df, col_name, code_list):\n",
    "    \"\"\"Takes a column and applies the \"\"\"\n",
    "    r = df[col_name].apply(lambda x: vectorize_icd_string(x, code_list))\n",
    "    r = np.transpose(np.column_stack(r))\n",
    "    return r\n",
    "\n",
    "\n",
    "# Vectorize and Pad notes Text\n",
    "\n",
    "def vectorize_notes(col, MAX_NB_WORDS, verbose = True):\n",
    "    \"\"\"Takes a note column and encodes it into a series of integer\n",
    "        Also returns the dictionnary mapping the word to the integer\"\"\"\n",
    "    tokenizer = Tokenizer(num_words = MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(col)\n",
    "    data = tokenizer.texts_to_sequences(col)\n",
    "    note_length =  [len(x) for x in data]\n",
    "    vocab = tokenizer.word_index\n",
    "    MAX_VOCAB = len(vocab)\n",
    "    if verbose:\n",
    "        print('Vocabulary size: %s' % MAX_VOCAB)\n",
    "        print('Average note length: %s' % np.mean(note_length))\n",
    "        print('Max note length: %s' % np.max(note_length))\n",
    "    return data, vocab, MAX_VOCAB\n",
    "\n",
    "def pad_notes(data, MAX_SEQ_LENGTH):\n",
    "    data = pad_sequences(data, maxlen = MAX_SEQ_LENGTH)\n",
    "    return data, data.shape[1]\n",
    "\n",
    "\n",
    "# Creates an embedding Matrix\n",
    "# Based on https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "\n",
    "def embedding_matrix(f_name, dictionary, EMBEDDING_DIM, verbose = True, sigma = None):\n",
    "    \"\"\"Takes a pre-trained embedding and adapts it to the dictionary at hand\n",
    "        Words not found will be all-zeros in the matrix\"\"\"\n",
    "\n",
    "    # Dictionary of words from the pre trained embedding\n",
    "    pretrained_dict = {}\n",
    "    with open(f_name, 'r') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            pretrained_dict[word] = coefs\n",
    "\n",
    "    # Default values for absent words\n",
    "    if sigma:\n",
    "        pretrained_matrix = sigma * np.random.rand(len(dictionary) + 1, EMBEDDING_DIM)\n",
    "    else:\n",
    "        pretrained_matrix = np.zeros((len(dictionary) + 1, EMBEDDING_DIM))\n",
    "    \n",
    "    # Substitution of default values by pretrained values when applicable\n",
    "    for word, i in dictionary.items():\n",
    "        vector = pretrained_dict.get(word)\n",
    "        if vector is not None:\n",
    "            pretrained_matrix[i] = vector\n",
    "\n",
    "    if verbose:\n",
    "        print('Vocabulary in notes:', len(dictionary))\n",
    "        print('Vocabulary in original embedding:', len(pretrained_dict))\n",
    "        inter = list( set(dictionary.keys()) & set(pretrained_dict.keys()) )\n",
    "        print('Vocabulary intersection:', len(inter))\n",
    "\n",
    "    return pretrained_matrix, pretrained_dict\n",
    "\n",
    "def train_val_test_split(X, y, val_size=0.2, test_size=0.2, random_state=101):\n",
    "    \"\"\"Splits the input and labels into 3 sets\"\"\"\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=(val_size+test_size), random_state=random_state)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=test_size/(val_size+test_size), random_state=random_state)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 150475\n",
      "Average note length: 1513.407647661318\n",
      "Max note length: 10500\n",
      "Final Vocabulary: 150475\n",
      "Final Max Sequence Length: 2500\n"
     ]
    }
   ],
   "source": [
    "#preprocess notes\n",
    "MAX_VOCAB = None # to limit original number of words (None if no limit)\n",
    "MAX_SEQ_LENGTH = 2500 # to limit length of word sequence (None if no limit)\n",
    "##TEXT is pre-processed\n",
    "data_vectorized, dictionary, MAX_VOCAB = vectorize_notes(df.TEXT, MAX_VOCAB, verbose = True)\n",
    "data, MAX_SEQ_LENGTH = pad_notes(data_vectorized, MAX_SEQ_LENGTH)\n",
    "\n",
    "print(\"Final Vocabulary: %s\" % MAX_VOCAB)\n",
    "print(\"Final Max Sequence Length: %s\" % MAX_SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 50 \n",
    "EMBEDDING_MATRIX= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary in notes: 150475\n",
      "Vocabulary in original embedding: 911413\n",
      "Vocabulary intersection: 59823\n"
     ]
    }
   ],
   "source": [
    "\n",
    "EMBEDDING_LOC = 'Emb50.txt' # pretrained 50 dimensional embeddings\n",
    "EMBEDDING_MATRIX, embedding_dict = embedding_matrix(EMBEDDING_LOC,\n",
    "                                                                  dictionary, EMBEDDING_DIM, verbose = True, sigma=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = ['blood', 'circulatory', 'congenital', 'digestive', 'endocrine',\n",
    "       'genitourinary', 'infectious', 'injury', 'mental', 'muscular',\n",
    "       'neoplasms', 'nervous', 'pregnancy', 'prenatal', 'respiratory', 'skin',\n",
    "       'symptoms', 'E and V']\n",
    "\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data\n",
    "Y = df[labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (36905, 2500) (36905, 18)\n",
      "Validation:  (10544, 2500) (10544, 18)\n",
      "Test:  (5273, 2500) (5273, 18)\n"
     ]
    }
   ],
   "source": [
    "#split sets\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(\n",
    "    X, Y, val_size=0.2, test_size=0.1, random_state=101)\n",
    "print(\"Train: \", X_train.shape, y_train.shape)\n",
    "print(\"Validation: \", X_val.shape, y_val.shape)\n",
    "print(\"Test: \", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete temporary variables to free some memory\n",
    "del df, data, X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNText classification\n",
    "\n",
    "Based on:\n",
    "* \"Convolutional Neural Networks for Sentence Classification\"   \n",
    "* http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/\n",
    "* https://github.com/alexander-rakhlin/CNN-for-Sentence-Classification-in-Keras/blob/master/sentiment_cnn.py\n",
    "* http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/\n",
    "* https://github.com/dennybritz/cnn-text-classification-tf/blob/master/text_cnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnntext_model(input_seq_length, \n",
    "                         max_vocab, external_embeddings, embedding_dim, embedding_matrix,\n",
    "                         num_filters, filter_sizes,\n",
    "                         training_dropout_keep_prob,\n",
    "                         num_classes):\n",
    "    #Embedding\n",
    "    model_input = Input(shape=(input_seq_length, ))\n",
    "    if external_embeddings:\n",
    "        # use embedding_matrix \n",
    "        z = Embedding(max_vocab + 1,\n",
    "                            embedding_dim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=input_seq_length,\n",
    "                            trainable=True)(model_input)\n",
    "    else:\n",
    "        # train embeddings \n",
    "        z =  Embedding(max_vocab + 1, \n",
    "                   embedding_dim, \n",
    "                   input_length=input_seq_length, embeddings_regularizer=regularizers.l2(0.0001),\n",
    "                   name=\"embedding\")(model_input)\n",
    "\n",
    "    # Convolutional block\n",
    "    conv_blocks = []\n",
    "    for sz in filter_sizes:\n",
    "        conv = Convolution1D(filters=num_filters,                         \n",
    "                         kernel_size=sz,\n",
    "                         padding=\"valid\",\n",
    "                         activation=\"relu\",\n",
    "                         strides=1)(z)\n",
    "        window_pool_size =  input_seq_length  - sz + 1 \n",
    "        conv = MaxPooling1D(pool_size=window_pool_size)(conv)  \n",
    "        conv = Flatten()(conv)\n",
    "        conv_blocks.append(conv)\n",
    "\n",
    "    #concatenate\n",
    "    z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
    "    z = Dropout(training_dropout_keep_prob)(z)\n",
    "\n",
    "    model_output = Dense(num_classes, activation=\"sigmoid\")(z)\n",
    "\n",
    "    model = Model(model_input, model_output)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 2500)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 2500, 50)     7523800     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 2499, 100)    10100       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 2498, 100)    15100       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 2497, 100)    20100       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 2496, 100)    25100       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 1, 100)       0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1, 100)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 1, 100)       0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 1, 100)       0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 100)          0           max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 100)          0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 100)          0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 100)          0           max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 400)          0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 400)          0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 18)           7218        dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 7,601,418\n",
      "Trainable params: 7,601,418\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#### build model\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "model = cnntext_model (input_seq_length=MAX_SEQ_LENGTH, max_vocab = MAX_VOCAB,\n",
    "                             external_embeddings = False,\n",
    "                             embedding_dim=EMBEDDING_DIM,embedding_matrix=EMBEDDING_MATRIX,\n",
    "                             num_filters = 100, filter_sizes=[2,3,4,5],\n",
    "                             training_dropout_keep_prob=0.5,\n",
    "                             num_classes=18 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2307/2307 - 284s - loss: 0.4517 - accuracy: 0.4383 - val_loss: 0.3903 - val_accuracy: 0.4189\n",
      "Epoch 2/200\n",
      "2307/2307 - 282s - loss: 0.3893 - accuracy: 0.3973 - val_loss: 0.3607 - val_accuracy: 0.4132\n",
      "Epoch 3/200\n",
      "2307/2307 - 283s - loss: 0.3725 - accuracy: 0.3909 - val_loss: 0.3515 - val_accuracy: 0.3947\n",
      "Epoch 4/200\n",
      "2307/2307 - 283s - loss: 0.3648 - accuracy: 0.3872 - val_loss: 0.3460 - val_accuracy: 0.3674\n",
      "Epoch 5/200\n",
      "2307/2307 - 283s - loss: 0.3579 - accuracy: 0.3822 - val_loss: 0.3426 - val_accuracy: 0.4008\n",
      "Epoch 6/200\n",
      "2307/2307 - 283s - loss: 0.3546 - accuracy: 0.3819 - val_loss: 0.3408 - val_accuracy: 0.3980\n",
      "Epoch 7/200\n",
      "2307/2307 - 282s - loss: 0.3524 - accuracy: 0.3799 - val_loss: 0.3394 - val_accuracy: 0.3957\n",
      "Epoch 8/200\n",
      "2307/2307 - 283s - loss: 0.3502 - accuracy: 0.3812 - val_loss: 0.3370 - val_accuracy: 0.3624\n",
      "Epoch 9/200\n",
      "2307/2307 - 283s - loss: 0.3487 - accuracy: 0.3781 - val_loss: 0.3377 - val_accuracy: 0.4144\n",
      "Epoch 10/200\n",
      "2307/2307 - 283s - loss: 0.3475 - accuracy: 0.3805 - val_loss: 0.3375 - val_accuracy: 0.4238\n",
      "Epoch 11/200\n",
      "2307/2307 - 283s - loss: 0.3466 - accuracy: 0.3818 - val_loss: 0.3371 - val_accuracy: 0.3879\n",
      "Epoch 12/200\n",
      "2307/2307 - 283s - loss: 0.3456 - accuracy: 0.3824 - val_loss: 0.3364 - val_accuracy: 0.3890\n",
      "Epoch 13/200\n",
      "2307/2307 - 290s - loss: 0.3447 - accuracy: 0.3789 - val_loss: 0.3369 - val_accuracy: 0.4230\n",
      "Epoch 14/200\n",
      "2307/2307 - 291s - loss: 0.3445 - accuracy: 0.3783 - val_loss: 0.3392 - val_accuracy: 0.4099\n",
      "Epoch 15/200\n",
      "2307/2307 - 290s - loss: 0.3440 - accuracy: 0.3782 - val_loss: 0.3381 - val_accuracy: 0.4209\n",
      "Epoch 16/200\n",
      "2307/2307 - 295s - loss: 0.3429 - accuracy: 0.3784 - val_loss: 0.3381 - val_accuracy: 0.4011\n",
      "Epoch 17/200\n",
      "2307/2307 - 291s - loss: 0.3427 - accuracy: 0.3801 - val_loss: 0.3395 - val_accuracy: 0.3748\n",
      "Epoch 18/200\n",
      "2307/2307 - 292s - loss: 0.3421 - accuracy: 0.3841 - val_loss: 0.3396 - val_accuracy: 0.4328\n",
      "Epoch 19/200\n",
      "2307/2307 - 294s - loss: 0.3419 - accuracy: 0.3821 - val_loss: 0.3376 - val_accuracy: 0.4114\n",
      "Epoch 20/200\n",
      "2307/2307 - 305s - loss: 0.3413 - accuracy: 0.3801 - val_loss: 0.3383 - val_accuracy: 0.4271\n",
      "Epoch 21/200\n",
      "2307/2307 - 287s - loss: 0.3407 - accuracy: 0.3803 - val_loss: 0.3378 - val_accuracy: 0.4313\n",
      "Epoch 22/200\n",
      "2307/2307 - 291s - loss: 0.3406 - accuracy: 0.3814 - val_loss: 0.3391 - val_accuracy: 0.4130\n",
      "Epoch 23/200\n",
      "2307/2307 - 297s - loss: 0.3397 - accuracy: 0.3807 - val_loss: 0.3380 - val_accuracy: 0.3883\n",
      "Epoch 24/200\n",
      "2307/2307 - 287s - loss: 0.3392 - accuracy: 0.3796 - val_loss: 0.3382 - val_accuracy: 0.3971\n",
      "Epoch 25/200\n",
      "2307/2307 - 291s - loss: 0.3390 - accuracy: 0.3821 - val_loss: 0.3376 - val_accuracy: 0.3914\n",
      "Epoch 26/200\n",
      "2307/2307 - 288s - loss: 0.3394 - accuracy: 0.3796 - val_loss: 0.3382 - val_accuracy: 0.4190\n",
      "Epoch 27/200\n",
      "2307/2307 - 294s - loss: 0.3380 - accuracy: 0.3796 - val_loss: 0.3381 - val_accuracy: 0.4091\n",
      "Epoch 28/200\n",
      "2307/2307 - 281s - loss: 0.3379 - accuracy: 0.3828 - val_loss: 0.3402 - val_accuracy: 0.3961\n",
      "Epoch 29/200\n",
      "2307/2307 - 284s - loss: 0.3381 - accuracy: 0.3806 - val_loss: 0.3394 - val_accuracy: 0.4170\n",
      "Epoch 30/200\n",
      "2307/2307 - 282s - loss: 0.3378 - accuracy: 0.3807 - val_loss: 0.3401 - val_accuracy: 0.4076\n",
      "Epoch 31/200\n",
      "2307/2307 - 284s - loss: 0.3380 - accuracy: 0.3786 - val_loss: 0.3411 - val_accuracy: 0.3979\n",
      "Epoch 32/200\n",
      "2307/2307 - 283s - loss: 0.3375 - accuracy: 0.3762 - val_loss: 0.3401 - val_accuracy: 0.4153\n",
      "Epoch 33/200\n",
      "2307/2307 - 283s - loss: 0.3374 - accuracy: 0.3823 - val_loss: 0.3404 - val_accuracy: 0.4089\n",
      "Epoch 34/200\n",
      "2307/2307 - 283s - loss: 0.3369 - accuracy: 0.3808 - val_loss: 0.3411 - val_accuracy: 0.4290\n",
      "Epoch 35/200\n",
      "2307/2307 - 284s - loss: 0.3362 - accuracy: 0.3834 - val_loss: 0.3404 - val_accuracy: 0.3890\n",
      "Epoch 36/200\n",
      "2307/2307 - 285s - loss: 0.3368 - accuracy: 0.3787 - val_loss: 0.3406 - val_accuracy: 0.3952\n",
      "Epoch 37/200\n",
      "2307/2307 - 282s - loss: 0.3356 - accuracy: 0.3866 - val_loss: 0.3410 - val_accuracy: 0.4106\n",
      "Epoch 38/200\n",
      "2307/2307 - 284s - loss: 0.3353 - accuracy: 0.3854 - val_loss: 0.3409 - val_accuracy: 0.4183\n",
      "Epoch 39/200\n",
      "2307/2307 - 284s - loss: 0.3361 - accuracy: 0.3863 - val_loss: 0.3420 - val_accuracy: 0.4214\n",
      "Epoch 40/200\n",
      "2307/2307 - 284s - loss: 0.3358 - accuracy: 0.3852 - val_loss: 0.3412 - val_accuracy: 0.3952\n",
      "Epoch 41/200\n",
      "2307/2307 - 284s - loss: 0.3349 - accuracy: 0.3878 - val_loss: 0.3425 - val_accuracy: 0.4333\n",
      "Epoch 42/200\n",
      "2307/2307 - 284s - loss: 0.3347 - accuracy: 0.3912 - val_loss: 0.3418 - val_accuracy: 0.4082\n",
      "Epoch 43/200\n",
      "2307/2307 - 284s - loss: 0.3349 - accuracy: 0.3905 - val_loss: 0.3452 - val_accuracy: 0.4226\n",
      "Epoch 44/200\n",
      "2307/2307 - 293s - loss: 0.3355 - accuracy: 0.3913 - val_loss: 0.3426 - val_accuracy: 0.3939\n",
      "Epoch 45/200\n",
      "2307/2307 - 292s - loss: 0.3353 - accuracy: 0.3887 - val_loss: 0.3431 - val_accuracy: 0.4038\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcb1c41d2b0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train, batch_size=16, epochs=200, validation_data=(X_val, y_val), verbose=2, callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test, batch_size=50)\n",
    "y_pred = np.where(pred > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "        blood     0.7688    0.5699    0.6546      1809\n",
      "  circulatory     0.9407    0.9436    0.9421      4132\n",
      "   congenital     0.7883    0.3942    0.5255       274\n",
      "    digestive     0.8525    0.7473    0.7965      2042\n",
      "    endocrine     0.8732    0.8987    0.8858      3534\n",
      "genitourinary     0.8671    0.7910    0.8273      2120\n",
      "   infectious     0.8172    0.7168    0.7637      1416\n",
      "       injury     0.8233    0.5969    0.6920      2225\n",
      "       mental     0.8227    0.6294    0.7132      1592\n",
      "     muscular     0.7225    0.4420    0.5485       966\n",
      "    neoplasms     0.8802    0.7730    0.8232       846\n",
      "      nervous     0.7776    0.6090    0.6831      1573\n",
      "    pregnancy     1.0000    0.4444    0.6154        18\n",
      "     prenatal     0.8090    0.5542    0.6578       848\n",
      "  respiratory     0.8709    0.8107    0.8397      2404\n",
      "         skin     0.7640    0.4474    0.5643       608\n",
      "     symptoms     0.6695    0.4776    0.5575      1629\n",
      "      E and V     0.7986    0.8996    0.8461      3655\n",
      "\n",
      "    micro avg     0.8391    0.7436    0.7885     31691\n",
      "    macro avg     0.8248    0.6525    0.7187     31691\n",
      " weighted avg     0.8329    0.7436    0.7800     31691\n",
      "  samples avg     0.8497    0.7491    0.7736     31691\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vithya/anaconda3/envs/tfgpu/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred,digits=4, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
